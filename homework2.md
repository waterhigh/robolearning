## 第一题：
首先对数据进行主成分分析（PCA），然后将每个样本的特征降维到1维和2维，再在2维的降维数据上进行重构回3维。

### 步骤：
1. **数据准备**：将类别1和类别2的样本组合成一个数据集。
2. **PCA降维**：使用PCA将数据分别降到1维和2维。
3. **位置计算**：得到每个样本在降至1维和2维后的新坐标。
4. **重构**：用降至2维的坐标重构回3维，得到每个样本的重构后的坐标。


我已展示了PCA降维至1维和2维后每个样本的位置，并在降至2维的数据上进行了重构，将其还原回3维空间坐标。可以查看表格了解每个样本的具体投影位置和重构结果。
```
import numpy as np
from sklearn.decomposition import PCA
import pandas as pd

# 定义类别1和类别2的数据
class1 = np.array([[0, 0, 0], [1, 0, 0], [1, 0, 1], [1, 1, 0]])
class2 = np.array([[0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 1, 1]])

# 合并数据进行PCA
data = np.vstack((class1, class2))

# 定义 PCA 对象并将数据降至1维
pca_1d = PCA(n_components=1)
data_1d = pca_1d.fit_transform(data)

# 将数据降至2维
pca_2d = PCA(n_components=2)
data_2d = pca_2d.fit_transform(data)

# 将2维数据重构回3维
reconstructed_3d_from_2d = pca_2d.inverse_transform(data_2d)

# 整理数据以便展示
df_results = pd.DataFrame({
    "Original": [tuple(point) for point in data],
    "1D Projection": data_1d.flatten(),
    "2D Projection": [tuple(point) for point in data_2d],
    "Reconstructed from 2D": [tuple(point) for point in reconstructed_3d_from_2d]
})

import ace_tools as tools; tools.display_dataframe_to_user(name="PCA降维及重构结果", dataframe=df_results)
```
| Original   |   1D Projection (Handwritten) | 2D Projection (Handwritten)   | Reconstructed from 2D (Handwritten)   |
|:-----------|------------------------------:|:------------------------------|:--------------------------------------|
| (0, 0, 0)  |                          -0.5 | (-0.5, -0.5)                  | (0.5, 0.0, 0.0)                       |
| (1, 0, 0)  |                          -0.5 | (-0.5, -0.5)                  | (0.5, 0.0, 0.0)                       |
| (1, 0, 1)  |                           0.5 | (0.5, -0.5)                   | (0.5, 0.0, 1.0)                       |
| (1, 1, 0)  |                          -0.5 | (-0.5, 0.5)                   | (0.5, 1.0, 0.0)                       |
| (0, 0, 1)  |                           0.5 | (0.5, -0.5)                   | (0.5, 0.0, 1.0)                       |
| (0, 1, 0)  |                          -0.5 | (-0.5, 0.5)                   | (0.5, 1.0, 0.0)                       |
| (0, 1, 1)  |                           0.5 | (0.5, 0.5)                    | (0.5, 1.0, 1.0)                       |
| (1, 1, 1)  |                           0.5 | (0.5, 0.5)                    | (0.5, 1.0, 1.0)                       |

## 第二题：

### 1. 模型的过拟合行为

当 ( $lambda = 0 $) 时，正则化项消失，目标函数简化为普通的最小二乘回归，此时：

- **模型容易发生过拟合**，在训练集上的拟合效果最好，但在新数据上表现较差。

随着 ( $lambda $) 增加，正则化项的权重逐渐增大，对模型复杂度的约束也逐渐增多，从而**减少了模型的过拟合**。

**总体趋势**：
- 随着 ( $lambda $) 增大，**模型的过拟合现象逐渐减弱**。
- 当 ( $lambda \to \infty $) 时，模型会趋于极端欠拟合，因为正则化项将 $( w $) 压缩至接近零，导致模型难以学习数据的特征。

### 2. $( w $) 的值和模的大小

目标函数中的正则化项会限制权重向量 $( w $) 的大小，从而控制模型复杂度。

- 当 ( $lambda = 0 $) 时，$( w $) 的模大小仅由数据决定，可能会很大（取决于数据的分布）。
- 随着 ( $lambda $) 增加，正则化项逐渐限制 $( w $) 的模大小，使其逐渐减小。
- 当 ( $lambda\to \infty $) 时，$( w $) 会非常接近零，模型逐渐趋于常数函数。

**总体趋势**：随着 ( $lambda $) 增大，**权重 $( w $) 的模大小逐渐减小**，最终在趋于无穷大时接近零。

### 3. 模型的偏差和方差

- **方差**：当 ( $lambda $) 增加时，模型复杂度降低，对训练数据的依赖性减少，因此**模型的方差逐渐减小**。低方差意味着模型对数据波动不敏感，更加稳定。
  
- **偏差**：随着 ( $lambda $) 增加，正则化限制变强，模型难以准确拟合数据，此时**偏差逐渐增大**，表现为欠拟合，预测偏离真实值。

**总体趋势**：随着 ( $lambda $) 增大，**模型的方差减小**，**偏差增大**，这体现了偏差-方差权衡原则：当 ( $lambda $) 较小时，方差较大，偏差较小；当 ( $lambda $) 较大时，方差较小，偏差较大。

## 第三题：
![手写](第三题.png)